{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Klasifikasi Data Review Google (Aplikasi Shopee)**\n",
        "---\n",
        "Proyek klasifikasi sentimen pada ulasan pengguna aplikasi Shopee dimulai dengan mengimpor library yang dibutuhkan dan mengeksplorasi dataset berisi teks ulasan beserta label sentimen (**positive, neutral, negative**). Setelah dilakukan pembersihan teks dan analisis distribusi kelas, ditemukan ketidakseimbangan data yang cukup signifikan, sehingga diterapkan teknik random oversampling untuk menyamakan jumlah data di setiap kelas. Data yang telah seimbang kemudian digunakan untuk pelatihan model dengan empat pendekatan berbeda, yaitu **Logistic Regression, Support Vector Machine (SVM), dan CNN-LSTM** sebagai metode deep learning. Evaluasi dilakukan menggunakan data uji dengan metrik accuracy, precision, recall, dan F1-score untuk menilai performa model dalam mengklasifikasikan ketiga jenis sentimen secara adil.\n",
        "\n",
        "🎯 **Proyek ini bertujuan untuk:**\n",
        "1. Membangun model klasifikasi yang dapat mengidentifikasi sentimen pengguna terhadap aplikasi Shopee,\n",
        "2. Membandingkan kinerja model tradisional vs deep learning,\n",
        "3. Mengatasi bias data melalui oversampling agar hasil prediksi adil dan seimbang di setiap kelas.\n"
      ],
      "metadata": {
        "id": "SP1k8mO0ePW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Import Library**"
      ],
      "metadata": {
        "id": "RBnA5GyVfUif"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "wZWAurxwdgdL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import sklearn\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQLSUWB1nH3c",
        "outputId": "1c380075-9c4f-4f8c-9a96-3e52bddc9659"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout"
      ],
      "metadata": {
        "id": "jnQ5GWJtnKTi"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Eksplorasi data**"
      ],
      "metadata": {
        "id": "LmhbHgTuiAq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# baca dataset hasil scrapping google\n",
        "url = 'shopee_reviews.csv'\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "IMwfj9vhfikG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlWSytChfvDH",
        "outputId": "871fcaaa-e8be-4786-da26-9c7dd25389e7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 37840 entries, 0 to 37839\n",
            "Data columns (total 18 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   reviewId              37840 non-null  object\n",
            " 1   userName              37840 non-null  object\n",
            " 2   userImage             37840 non-null  object\n",
            " 3   content               37840 non-null  object\n",
            " 4   score                 37840 non-null  int64 \n",
            " 5   thumbsUpCount         37840 non-null  int64 \n",
            " 6   reviewCreatedVersion  37840 non-null  object\n",
            " 7   at                    37840 non-null  object\n",
            " 8   replyContent          37840 non-null  object\n",
            " 9   repliedAt             37840 non-null  object\n",
            " 10  appVersion            37840 non-null  object\n",
            " 11  text_clean            37840 non-null  object\n",
            " 12  text_slangwords       37840 non-null  object\n",
            " 13  text_tokenizing       37840 non-null  object\n",
            " 14  text_stopword         37840 non-null  object\n",
            " 15  text_akhir            37840 non-null  object\n",
            " 16  polarity_score        37840 non-null  int64 \n",
            " 17  polarity              37840 non-null  object\n",
            "dtypes: int64(3), object(15)\n",
            "memory usage: 5.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = df.copy()\n",
        "\n",
        "# Menghapus baris yang mengandung NaN dan data duplikat\n",
        "clean_df = clean_df.dropna()\n",
        "clean_df = clean_df.drop_duplicates()\n",
        "\n",
        "print(\"Jumlah baris dan kolom setelah cleaning:\", clean_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJNvKn8cheX2",
        "outputId": "eef3b5f8-8937-4942-e0bd-036345f8dc56"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah baris dan kolom setelah cleaning: (37840, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw9Y6Yt2hgdW",
        "outputId": "780030c2-883f-466d-a434-34b5bdd23e8c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 37840 entries, 0 to 37839\n",
            "Data columns (total 18 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   reviewId              37840 non-null  object\n",
            " 1   userName              37840 non-null  object\n",
            " 2   userImage             37840 non-null  object\n",
            " 3   content               37840 non-null  object\n",
            " 4   score                 37840 non-null  int64 \n",
            " 5   thumbsUpCount         37840 non-null  int64 \n",
            " 6   reviewCreatedVersion  37840 non-null  object\n",
            " 7   at                    37840 non-null  object\n",
            " 8   replyContent          37840 non-null  object\n",
            " 9   repliedAt             37840 non-null  object\n",
            " 10  appVersion            37840 non-null  object\n",
            " 11  text_clean            37840 non-null  object\n",
            " 12  text_slangwords       37840 non-null  object\n",
            " 13  text_tokenizing       37840 non-null  object\n",
            " 14  text_stopword         37840 non-null  object\n",
            " 15  text_akhir            37840 non-null  object\n",
            " 16  polarity_score        37840 non-null  int64 \n",
            " 17  polarity              37840 non-null  object\n",
            "dtypes: int64(3), object(15)\n",
            "memory usage: 5.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cek data polarity apakah setiap data seimbang\n",
        "print(clean_df['polarity'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5ooOQhVhj6r",
        "outputId": "f0162736-0773-4983-8067-30121388f3e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "polarity\n",
            "positive    19773\n",
            "negative    15616\n",
            "neutral      2451\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Penanganan Ketidakseimbangan Data (Oversampling)**"
      ],
      "metadata": {
        "id": "_dU8LSfJhx_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Pisahkan masing-masing kelas\n",
        "neutral_df = clean_df[clean_df['polarity'] == 'neutral']\n",
        "positive_df = clean_df[clean_df['polarity'] == 'positive']\n",
        "negative_df = clean_df[clean_df['polarity'] == 'negative']\n",
        "\n",
        "# Oversample kelas neutral ke jumlah target (misal samakan dengan positive: 12500)\n",
        "neutral_oversampled = resample(\n",
        "    neutral_df,\n",
        "    replace=True,           # sampling dengan pengembalian\n",
        "    n_samples=12500,        # target jumlah\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Gabungkan kembali semua\n",
        "balanced_df = pd.concat([positive_df, negative_df, neutral_oversampled])\n",
        "\n",
        "# Cek distribusinya\n",
        "print(balanced_df['polarity'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q-EO-vghxe5",
        "outputId": "b8ee9375-8fed-4b4e-eeb2-88e6e581f81c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "polarity\n",
            "positive    19773\n",
            "negative    15616\n",
            "neutral     12500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi encoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Ubah label string ke bentuk angka\n",
        "balanced_df[\"label\"] = le.fit_transform(balanced_df[\"polarity\"])\n",
        "print(dict(zip(le.classes_, le.transform(le.classes_))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWdwSQqCihm9",
        "outputId": "6482a8c8-7fd8-4452-ef91-a451ddf2d044"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'negative': np.int64(0), 'neutral': np.int64(1), 'positive': np.int64(2)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYg2_cStiawK",
        "outputId": "1ff57644-4174-4537-c847-3370332b23b5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 47889 entries, 5 to 19556\n",
            "Data columns (total 19 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   reviewId              47889 non-null  object\n",
            " 1   userName              47889 non-null  object\n",
            " 2   userImage             47889 non-null  object\n",
            " 3   content               47889 non-null  object\n",
            " 4   score                 47889 non-null  int64 \n",
            " 5   thumbsUpCount         47889 non-null  int64 \n",
            " 6   reviewCreatedVersion  47889 non-null  object\n",
            " 7   at                    47889 non-null  object\n",
            " 8   replyContent          47889 non-null  object\n",
            " 9   repliedAt             47889 non-null  object\n",
            " 10  appVersion            47889 non-null  object\n",
            " 11  text_clean            47889 non-null  object\n",
            " 12  text_slangwords       47889 non-null  object\n",
            " 13  text_tokenizing       47889 non-null  object\n",
            " 14  text_stopword         47889 non-null  object\n",
            " 15  text_akhir            47889 non-null  object\n",
            " 16  polarity_score        47889 non-null  int64 \n",
            " 17  polarity              47889 non-null  object\n",
            " 18  label                 47889 non-null  int64 \n",
            "dtypes: int64(4), object(15)\n",
            "memory usage: 7.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Perbandingan Logistic Regression, SVM, dan CNN-LSTM untuk Klasifikasi Sentimen Multikelas**"
      ],
      "metadata": {
        "id": "J5ao1xJci_pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_balanced = balanced_df['text_akhir']\n",
        "y_balanced = balanced_df['polarity']"
      ],
      "metadata": {
        "id": "Mh4-efgXkCwU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **SKEMA 1**\n",
        "###### **PELATIHAN DAN EVALUASI MODEL LOGISTIC REGRESSION CLASSIFICATION DENGAN OVERSAMPLING**\n",
        "---\n",
        "Berikut skema model machine learning untuk klasifikasi teks menggunakan algoritma Logistic Regression, yang dipadukan dengan teknik ekstraksi fitur TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "\n",
        "Pelatihan: **Logistic Regression**\n",
        "\n",
        "Ekstraksi Fitur: **TF-IDF**\n",
        "\n",
        "Pembagian data: **70/30**"
      ],
      "metadata": {
        "id": "30WCR_VOivcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Split data 70/30\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
        "\n",
        "# 2. Ekstraksi fitur menggunakan TF-IDF\n",
        "tfidf_vectorizer_logis = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = tfidf_vectorizer_logis.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer_logis.transform(X_test)\n",
        "\n",
        "# 3. Pelatihan model Logistic Regression\n",
        "model_logis_tf = LogisticRegression(\n",
        "    solver='sag',\n",
        "    max_iter=1000,\n",
        "    random_state=42)\n",
        "model_logis_tf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 4. Prediksi dan evaluasi\n",
        "y_pred_train = model_logis_tf.predict(X_train_tfidf)\n",
        "y_pred_test = model_logis_tf.predict(X_test_tfidf)\n",
        "\n",
        "# 5. Evaluasi akurasi\n",
        "print(\"Train Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
        "\n",
        "# 6. Classification Report\n",
        "print(\"\\nClassification Report on Test Set:\\n\")\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_KSMVk3i6rN",
        "outputId": "5abe6aef-c714-48b4-f287-47049dc62560"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9193663862538035\n",
            "Test Accuracy: 0.8681701120623652\n",
            "\n",
            "Classification Report on Test Set:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.88      0.87      0.87      4685\n",
            "     neutral       0.80      0.82      0.81      3750\n",
            "    positive       0.90      0.90      0.90      5932\n",
            "\n",
            "    accuracy                           0.87     14367\n",
            "   macro avg       0.86      0.86      0.86     14367\n",
            "weighted avg       0.87      0.87      0.87     14367\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model dan vectorizer\n",
        "joblib.dump(model_logis_tf, 'logistic_tf_model.joblib')\n",
        "joblib.dump(tfidf_vectorizer_logis, 'tfidf_vectorizer_logis.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tRaon6-pTn3",
        "outputId": "16052de9-7b17-4dbc-a992-34c36ead5764"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer_logis.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **SKEMA 2**\n",
        "###### **PELATIHAN DAN EVALUASI MODEL LOGISTIC REGRESSION CLASSIFICATION DENGAN OVERSAMPLING**\n",
        "---\n",
        "Berikut skema model machine learning untuk klasifikasi teks menggunakan algoritma Logistic Regression yang dipadukan dengan teknik ekstraksi fitur Bag-of-Words (BoW)\n",
        "\n",
        "Pelatihan: **Logistic Regression**\n",
        "\n",
        "Ekstraksi Fitur: **BoW**\n",
        "\n",
        "Pembagian data: **80/20**"
      ],
      "metadata": {
        "id": "PsxsHqVgk98r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced)\n",
        "\n",
        "# 2. Inisialisasi CountVectorizer (BoW)\n",
        "bow_vectorizer = CountVectorizer(\n",
        "    max_features=30000,\n",
        "    ngram_range=(1, 1),\n",
        "    stop_words='english'\n",
        ")\n",
        "\n",
        "# 3. Ekstraksi fitur\n",
        "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
        "X_test_bow = bow_vectorizer.transform(X_test)\n",
        "\n",
        "# 4. Latih model klasifikasi\n",
        "model_logis_bow = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_logis_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# 5. Prediksi dan evaluasi\n",
        "y_pred_train = model_logis_bow.predict(X_train_bow)\n",
        "y_pred_test = model_logis_bow.predict(X_test_bow)\n",
        "\n",
        "# 6. Evaluasi akurasi\n",
        "print(\"Train Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
        "\n",
        "# 7. Classification Report\n",
        "print(\"\\nClassification Report on Test Set:\\n\")\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkJcU8BFlKrj",
        "outputId": "70d2570e-f691-436e-ab21-3e6e50599d0d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9628044164861267\n",
            "Test Accuracy: 0.9023804552098559\n",
            "\n",
            "Classification Report on Test Set:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.95      0.85      0.90      3123\n",
            "     neutral       0.80      0.96      0.87      2500\n",
            "    positive       0.94      0.91      0.93      3955\n",
            "\n",
            "    accuracy                           0.90      9578\n",
            "   macro avg       0.90      0.91      0.90      9578\n",
            "weighted avg       0.91      0.90      0.90      9578\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model dan vectorizer\n",
        "joblib.dump(model_logis_bow, 'logistic_bow_model.joblib')\n",
        "joblib.dump(bow_vectorizer, 'bow_vectorizer.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iKe7QXmto_H",
        "outputId": "4bae7307-74f0-4092-9504-3a4003d41773"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bow_vectorizer.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **SKEMA 3**\n",
        "###### **PELATIHAN DAN EVALUASI MODEL LINEAR SUPPORT VECTOR CLASSIFICATION**\n",
        "---\n",
        "\n",
        "Berikut skema model machine learning untuk klasifikasi teks menggunakan algoritma Linear Support Vector Classification (LinearSVC) yang dipadukan dengan teknik ekstraksi fitur TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "\n",
        "Pelatihan: **SVM**\n",
        "\n",
        "Ekstraksi Fitur: **TF-IDF**\n",
        "\n",
        "Pembagian data: **80/20**"
      ],
      "metadata": {
        "id": "f5RqLJOUlXyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    x_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n",
        ")\n",
        "\n",
        "# 2. Pipeline: TF-IDF + LinearSVC\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(\n",
        "        max_features=10000,\n",
        "        ngram_range=(1, 3),\n",
        "        min_df=5,\n",
        "        stop_words='english'\n",
        "    )),\n",
        "    ('svc', LinearSVC(\n",
        "        class_weight='balanced',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 3. Hyperparameter tuning\n",
        "param_grid = {\n",
        "    'svc__C': [0.1, 0.5, 1, 2, 5, 10],\n",
        "    'svc__tol': [1e-3, 1e-4],\n",
        "    'svc__max_iter': [1000, 2000]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 4. Training\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Evaluasi model terbaik\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best CV Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 6. Evaluasi di training set\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "# 7. Evaluasi di test set\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# 8. Output hasil evaluasi\n",
        "print(\"\\nTrain Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hyhbw-ulfzo",
        "outputId": "9eadd711-6a19-4610-d90e-26834473dced"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "Best Parameters: {'svc__C': 10, 'svc__max_iter': 1000, 'svc__tol': 0.001}\n",
            "Best CV Accuracy: 0.9120878655220113\n",
            "\n",
            "Train Accuracy: 0.9939181958184333\n",
            "Test Accuracy: 0.9155356024222175\n",
            "\n",
            "Classification Report on Test Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.93      0.87      0.90      3123\n",
            "     neutral       0.87      0.98      0.92      2500\n",
            "    positive       0.94      0.91      0.92      3955\n",
            "\n",
            "    accuracy                           0.92      9578\n",
            "   macro avg       0.91      0.92      0.91      9578\n",
            "weighted avg       0.92      0.92      0.92      9578\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan pipeline terbaik\n",
        "joblib.dump(best_model, 'best_svc_tfidf_pipeline.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bes-TF-luMY5",
        "outputId": "c005adf8-93bd-4db9-db25-780e300545b6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best_svc_tfidf_pipeline.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **SKEMA 4**\n",
        "###### **PELATIHAN DAN EVALUASI MODEL CNN-LSTM DENGAN WORD EMBEDDING**\n",
        "---\n",
        "Berikut skema model deep learning untuk klasifikasi teks menggunakan arsitektur gabungan Convolutional Neural Network (CNN) dan Long Short-Term Memory (LSTM), yang dipadukan dengan teknik ekstraksi fitur berbasis Word Embedding menggunakan Keras.\n",
        "\n",
        "Pelatihan: **CNN-LSTM**\n",
        "\n",
        "Ekstraksi Fitur: **Word Embedding (Keras Embedding Layer)**\n",
        "\n",
        "Pembagian data: **70/30**\n",
        "\n",
        "Fungsi Aktivasi Output: **Softmax**\n",
        "\n",
        "Fungsi Loss: **Categorical Crossentropy**\n",
        "\n",
        "Optimizer: **Adam**\n",
        "\n",
        "Model ini mampu menangkap fitur lokal dari teks melalui lapisan konvolusi, kemudian mengolah informasi sekuensial menggunakan LSTM untuk prediksi kelas akhir (positif, netral, negatif). Arsitektur ini cocok untuk tugas klasifikasi teks multi-kelas."
      ],
      "metadata": {
        "id": "BkoO6oDzm742"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data (stratify biar seimbang)\n",
        "train_df, test_df = train_test_split(\n",
        "    balanced_df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=balanced_df[\"label\"]\n",
        ")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_df[\"text_akhir\"])\n",
        "\n",
        "# Konversi ke urutan angka\n",
        "X_train_seq = tokenizer.texts_to_sequences(train_df[\"text_akhir\"])\n",
        "X_test_seq = tokenizer.texts_to_sequences(test_df[\"text_akhir\"])\n",
        "\n",
        "# Padding\n",
        "max_len = 100  # bisa disesuaikan\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "# One-hot encode label\n",
        "y_train = to_categorical(train_df[\"label\"])\n",
        "y_test = to_categorical(test_df[\"label\"])\n",
        "\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Embedding(input_dim=10000, output_dim=128, input_length=max_len))\n",
        "model_cnn.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_cnn.add(LSTM(units=64, return_sequences=False))\n",
        "model_cnn.add(Dropout(0.5))\n",
        "model_cnn.add(Dense(3, activation='softmax'))  # 3 kelas\n",
        "\n",
        "model_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB7m2HvUnE5L",
        "outputId": "416f52fd-8d6d-4201-a812-e00e7904ef27"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_cnn.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_test_pad, y_test),\n",
        "    epochs=5,\n",
        "    batch_size=64\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfx-7c2vnVQB",
        "outputId": "e8fa20b7-97a2-4618-d9e1-3a18d9151039"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 123ms/step - accuracy: 0.5213 - loss: 0.9388 - val_accuracy: 0.8469 - val_loss: 0.4225\n",
            "Epoch 2/5\n",
            "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 122ms/step - accuracy: 0.8906 - loss: 0.3132 - val_accuracy: 0.8978 - val_loss: 0.2899\n",
            "Epoch 3/5\n",
            "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 118ms/step - accuracy: 0.9542 - loss: 0.1405 - val_accuracy: 0.9217 - val_loss: 0.2571\n",
            "Epoch 4/5\n",
            "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 119ms/step - accuracy: 0.9707 - loss: 0.0968 - val_accuracy: 0.9234 - val_loss: 0.2880\n",
            "Epoch 5/5\n",
            "\u001b[1m599/599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 121ms/step - accuracy: 0.9810 - loss: 0.0646 - val_accuracy: 0.9238 - val_loss: 0.2366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_cnn.predict(X_test_pad)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred_classes))\n",
        "target_names = [\"negative\", \"neutral\", \"positive\"]\n",
        "print(classification_report(y_true, y_pred_classes, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBmAMZa3nYzd",
        "outputId": "80d2d5ac-6cd8-4f5a-9825-313e063860d6"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step\n",
            "Accuracy: 0.9237836709125078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.96      0.89      0.93      3123\n",
            "     neutral       0.91      0.91      0.91      2500\n",
            "    positive       0.91      0.96      0.93      3955\n",
            "\n",
            "    accuracy                           0.92      9578\n",
            "   macro avg       0.93      0.92      0.92      9578\n",
            "weighted avg       0.93      0.92      0.92      9578\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Langsung simpan\n",
        "import pickle\n",
        "\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "model_cnn.save(\"cnn_lstm_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaGTCrqQu-Bc",
        "outputId": "1e3e2050-5f0d-4cc6-f456-fbdbd8a456ca"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Uji Validitas Model Menggunakan Data Teks Baru**"
      ],
      "metadata": {
        "id": "Xrz-w8wHvqfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teks_baru = [\n",
        "    \"Saya benci ini sangat mengecewakan\",\n",
        "    \"Barang yang saya terima rusak dan sangat mengecewakan\",\n",
        "    \"Saya baru menerima paketnya hari ini\",\n",
        "    \"Produk sesuai dskripsi\",\n",
        "    \"Belum sempat digunakan\"\n",
        "]\n",
        "print(teks_baru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d64OszwvrWl",
        "outputId": "1d663313-2c9c-434e-87c7-80e60f497e98"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Saya benci ini sangat mengecewakan', 'Barang yang saya terima rusak dan sangat mengecewakan', 'Saya baru menerima paketnya hari ini', 'Produk sesuai dskripsi', 'Belum sempat digunakan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform dan prediksi model Logistic Regression (TF-IDF)\n",
        "print(\"--- Hasil Prediksi Model Logistic Regression (TF-IDF) ---\")\n",
        "teks_baru_tfidf = tfidf_vectorizer_logis.transform(teks_baru)\n",
        "prediksi_kelas = model_logis_tf.predict(teks_baru_tfidf)\n",
        "\n",
        "for teks, label in zip(teks_baru, prediksi_kelas):\n",
        "    print(f\"Teks: {teks}\\nPrediksi Label: {label}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCDdBK3dxO1Z",
        "outputId": "39744e36-78bc-43f4-8537-238a58cf7317"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Hasil Prediksi Model Logistic Regression (TF-IDF) ---\n",
            "Teks: Saya benci ini sangat mengecewakan\n",
            "Prediksi Label: negative\n",
            "\n",
            "Teks: Barang yang saya terima rusak dan sangat mengecewakan\n",
            "Prediksi Label: negative\n",
            "\n",
            "Teks: Saya baru menerima paketnya hari ini\n",
            "Prediksi Label: neutral\n",
            "\n",
            "Teks: Produk sesuai dskripsi\n",
            "Prediksi Label: positive\n",
            "\n",
            "Teks: Belum sempat digunakan\n",
            "Prediksi Label: neutral\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform dan prediksi model Logistic Regression (BoW)\n",
        "print(\"--- Hasil Prediksi Model Logistic Regression (BoW) ---\")\n",
        "X_new = bow_vectorizer.transform(teks_baru)\n",
        "y_pred_logis_bow = model_logis_bow.predict(X_new)\n",
        "\n",
        "for teks, label in zip(teks_baru, y_pred_logis_bow):\n",
        "    print(f\"Teks: {teks}\\nPrediksi Label: {label}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElFNwFuDvvlv",
        "outputId": "3dcb41ec-bdaf-4d9f-8bab-c8f362861035"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Hasil Prediksi Model Logistic Regression (BoW) ---\n",
            "Teks: Saya benci ini sangat mengecewakan\n",
            "Prediksi Label: negative\n",
            "\n",
            "Teks: Barang yang saya terima rusak dan sangat mengecewakan\n",
            "Prediksi Label: negative\n",
            "\n",
            "Teks: Saya baru menerima paketnya hari ini\n",
            "Prediksi Label: neutral\n",
            "\n",
            "Teks: Produk sesuai dskripsi\n",
            "Prediksi Label: positive\n",
            "\n",
            "Teks: Belum sempat digunakan\n",
            "Prediksi Label: neutral\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform dan prediksi model SVM (TF-IDF + SVC via pipeline)\n",
        "print(\"--- Hasil Prediksi Model SVM ---\")\n",
        "y_pred_svm = best_model.predict(teks_baru)\n",
        "for teks, label in zip(teks_baru, y_pred_svm):\n",
        "    print(f\"Teks: {teks}\\nPrediksi Label: {label}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNyfA5dzwOJm",
        "outputId": "e06935fb-f660-495b-c6ed-182d5438e800"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Hasil Prediksi Model SVM ---\n",
            "Teks: Saya benci ini sangat mengecewakan\n",
            "Prediksi Label: negative\n",
            "\n",
            "Teks: Barang yang saya terima rusak dan sangat mengecewakan\n",
            "Prediksi Label: negative\n",
            "\n",
            "Teks: Saya baru menerima paketnya hari ini\n",
            "Prediksi Label: positive\n",
            "\n",
            "Teks: Produk sesuai dskripsi\n",
            "Prediksi Label: positive\n",
            "\n",
            "Teks: Belum sempat digunakan\n",
            "Prediksi Label: neutral\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisasi\n",
        "sequences_baru = tokenizer.texts_to_sequences(teks_baru)\n",
        "\n",
        "# Padding\n",
        "X_baru_pad = pad_sequences(sequences_baru, maxlen=max_len, padding='post')\n",
        "\n",
        "y_pred_probs = model_cnn.predict(X_baru_pad)  # hasil: probabilitas\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "label_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "print(\"--- Hasil Prediksi Model CNN ---\")\n",
        "for teks, pred in zip(teks_baru, y_pred_classes):\n",
        "    label = label_mapping[pred]  # ambil nama label dari mapping\n",
        "    print(f\"Teks: {teks}\\nPrediksi Label: {label}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1KTn8MdyMU2",
        "outputId": "8b74a6e2-4524-41e6-c08e-191fddf1a8ff"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
            "--- Hasil Prediksi Model CNN ---\n",
            "Teks: Saya benci ini sangat mengecewakan\n",
            "Prediksi Label: negative\n",
            "\n",
            "Teks: Barang yang saya terima rusak dan sangat mengecewakan\n",
            "Prediksi Label: negative\n",
            "\n",
            "Teks: Saya baru menerima paketnya hari ini\n",
            "Prediksi Label: positive\n",
            "\n",
            "Teks: Produk sesuai dskripsi\n",
            "Prediksi Label: positive\n",
            "\n",
            "Teks: Belum sempat digunakan\n",
            "Prediksi Label: positive\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Kesimpulan**\n",
        "\n",
        "---\n",
        "\n",
        "Dari hasil evaluasi empat model klasifikasi sentimen, CNN menunjukkan performa terbaik dengan akurasi tertinggi (92,4%) dan f1-score yang seimbang di semua kelas. SVM menempati posisi kedua dengan performa yang sangat stabil dan akurasi 91,5%. Logistic Regression dengan BoW juga cukup andal (90,2%) dan cocok untuk penggunaan ringan. Sementara itu, model TF-IDF memiliki akurasi paling rendah (86,8%) dan cenderung kurang akurat untuk kelas netral. Namun, untuk prediksi teks baru, Logistic Regression (TF-IDF dan BoW) justru lebih konsisten memahami konteks netral dan ambigu. CNN direkomendasikan untuk performa maksimal, SVM untuk keseimbangan, dan BoW untuk efisiensi."
      ],
      "metadata": {
        "id": "pL2haXdM0qjv"
      }
    }
  ]
}